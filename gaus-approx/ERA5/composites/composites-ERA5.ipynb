{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83cabd42",
   "metadata": {},
   "source": [
    "# Composite maps\n",
    "\n",
    "In this notebook we analyze the composite maps computed on the ERA5 dataset.\n",
    "\n",
    "If you haven't downloaded the data yet, follow the instructions in `Data_ERA5/README.md` and `Data_ERA5/preprocess.ipynb`\n",
    "\n",
    "If you have already computed the composites you can skip to section \"Compute metrics\"\n",
    "\n",
    "If you have already computed the metrics, you can skip to \"Analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563e12d-e92b-420f-b84f-658db22467fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "matplotlib.rc('font', size=18)\n",
    "import pandas as pd\n",
    "default_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../Climate-Learning/')\n",
    "import general_purpose.utilities as ut\n",
    "\n",
    "import general_purpose.tables as tbl\n",
    "\n",
    "HOME = '../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86372109-90ad-496a-9b52-b1755625c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(x, **kwargs):\n",
    "    return np.sqrt(np.sum(x**2, **kwargs))\n",
    "\n",
    "def split_acf(A,lag):\n",
    "    return np.corrcoef(A[...,lag:].flatten(), A[...,:-lag].flatten())[0,1] if lag else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2fe1a3-057e-470c-97ba-e75177bfc754",
   "metadata": {},
   "source": [
    "## Compute composites\n",
    "\n",
    "Run the script `compute_composites_ERA5.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5a6f4-fd05-46a7-8fc7-d7721ed28a8c",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f885373-554e-47c7-b868-d20fae18de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'ERA5/y83'\n",
    "\n",
    "Model = 'ERA5'\n",
    "\n",
    "# Northern Hemisphere\n",
    "sector = ''\n",
    "mask = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9a241-e7bb-45b3-af94-5787e5d64465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "item = {}\n",
    "reshaper = None\n",
    "\n",
    "lat = np.load(f'{root_folder}/lat.npy')\n",
    "coslat = np.cos(lat*np.pi/180)\n",
    "for s in tqdm(os.listdir(root_folder)):\n",
    "    if not s.startswith('T'):\n",
    "        continue\n",
    "    item['T'] = int(s[1:])\n",
    "    f = f'{root_folder}/{s}'\n",
    "    A = np.load(f'{f}/A.npy')\n",
    "    item['r'] = r = split_acf(A,1)\n",
    "    item['T_decorr'] = int((1+r)/(1-r) + 0.5)\n",
    "    for ss in os.listdir(f):\n",
    "        if not ss.startswith('tau'):\n",
    "            continue\n",
    "        item['tau'] = int(ss[3:])\n",
    "        ff = f'{f}/{ss}'\n",
    "        X_std = np.load(f'{ff}/X_std.npy') * mask\n",
    "        \n",
    "        field_dimensions = tuple(range(len(X_std.shape) - 1)) # number of dimensions for each field (should be 2, lon and lat)\n",
    "        pixels_per_field = np.sum(X_std > 0, axis=field_dimensions)\n",
    "        X_std_fm = np.sqrt(np.sum(X_std**2, axis=field_dimensions)/pixels_per_field)\n",
    "        assert X_std_fm.shape == X_std.shape[-1:], 'Shape mismatch!'\n",
    "        \n",
    "        area_weights = (np.ones_like(X_std).T * coslat).T\n",
    "        area_weights *= (X_std > 0)\n",
    "        area_weights /= np.sum(area_weights)\n",
    "        fieldwise_area_weights = area_weights / np.sum(area_weights, axis=field_dimensions)\n",
    "        \n",
    "        assert np.allclose(np.sum(area_weights), 1), 'Not normalized area weights'\n",
    "        assert np.allclose(np.sum(fieldwise_area_weights, axis=field_dimensions), 1), 'Not normalized fieldwise area weights'\n",
    "        \n",
    "        if reshaper is None:\n",
    "            reshaper = ut.Reshaper(X_std != 0)\n",
    "            print(reshaper.surviving_coords)\n",
    "        for sss in os.listdir(ff):\n",
    "            if not sss.startswith('percent'):\n",
    "                continue\n",
    "            fff = f'{ff}/{sss}'\n",
    "            item['percent'] = float(sss[7:])\n",
    "            item['a'] = np.load(f'{fff}/threshold.npy')[0]\n",
    "            try:\n",
    "                # raise FileNotFoundError()\n",
    "                nr = np.load(f'{fff}/{sector}norm_ratio.npy')\n",
    "                nrgn = np.load(f'{fff}/{sector}norm_ratio_global_normalization.npy')\n",
    "                n = np.load(f'{fff}/{sector}norm.npy')\n",
    "                en = np.load(f'{fff}/{sector}error_norm.npy')\n",
    "                for _ in [nr, nrgn, n, en]:\n",
    "                    if _.shape != X_std.shape[-1] + 1:\n",
    "                        raise FileNotFoundError()\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                # compute norm ratio\n",
    "                nr = np.zeros(X_std.shape[-1] + 1)\n",
    "                n = np.zeros(X_std.shape[-1] + 1)\n",
    "                en = np.zeros(X_std.shape[-1] + 1)\n",
    "                comp = np.load(f'{fff}/X_comp.npy')*mask\n",
    "                comp_ga = np.load(f'{fff}/X_comp_GA.npy')*mask\n",
    "                \n",
    "                n[:-1] = l2(comp, axis=field_dimensions)#/np.sqrt(pixels_per_field)\n",
    "                n[-1] = l2(comp)#/np.sqrt(np.sum(pixels_per_field))\n",
    "                np.save(f'{fff}/{sector}norm.npy', n)\n",
    "                en[:-1] = l2(comp - comp_ga, axis=field_dimensions)#/np.sqrt(pixels_per_field)\n",
    "                en[-1] = l2(comp - comp_ga)#/np.sqrt(np.sum(pixels_per_field))\n",
    "                np.save(f'{fff}/{sector}error_norm.npy', en)\n",
    "                nr[:-1] = l2(comp - comp_ga, axis=field_dimensions)/l2(comp, axis=field_dimensions)\n",
    "                nr[-1] = l2(comp - comp_ga)/l2(comp)\n",
    "                np.save(f'{fff}/{sector}norm_ratio.npy', nr)\n",
    "                \n",
    "                nrgn = np.zeros(X_std.shape[-1] + 1)\n",
    "                comp *= X_std/X_std_fm\n",
    "                comp_ga *= X_std/X_std_fm\n",
    "                nrgn[:-1] = l2(comp - comp_ga, axis=field_dimensions)/l2(comp, axis=field_dimensions)\n",
    "                nrgn[-1] = l2(comp - comp_ga)/l2(comp)\n",
    "                np.save(f'{fff}/{sector}norm_ratio_global_normalization.npy', nrgn)\n",
    "            \n",
    "            item['total_norm_ratio'] = nr[-1]\n",
    "            item['fieldwise_norm_ratio'] = nr[:-1]\n",
    "            item['total_norm_ratio_gn'] = nrgn[-1]\n",
    "            item['fieldwise_norm_ratio_gn'] = nrgn[:-1]\n",
    "            item['total_norm'] = n[-1]\n",
    "            item['fieldwise_norm'] = n[:-1]\n",
    "            item['total_error_norm'] = en[-1]\n",
    "            item['fieldwise_error_norm'] = en[:-1]\n",
    "            \n",
    "            # area weighted metrics\n",
    "            try:\n",
    "                # raise FileNotFoundError()\n",
    "                aw_nr = np.load(f'{fff}/{sector}aw_norm_ratio.npy')\n",
    "                aw_nrgn = np.load(f'{fff}/{sector}aw_norm_ratio_global_normalization.npy')\n",
    "                aw_n = np.load(f'{fff}/{sector}aw_norm.npy')\n",
    "                aw_en = np.load(f'{fff}/{sector}aw_error_norm.npy')\n",
    "                for _ in [aw_nr, aw_nrgn, aw_n, aw_en]:\n",
    "                    if _.shape != X_std.shape[-1] + 1:\n",
    "                        raise FileNotFoundError()\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                # compute norm ratio\n",
    "                aw_nr = np.zeros(X_std.shape[-1] + 1)\n",
    "                aw_n = np.zeros(X_std.shape[-1] + 1)\n",
    "                aw_en = np.zeros(X_std.shape[-1] + 1)\n",
    "                comp = np.load(f'{fff}/X_comp.npy')*mask\n",
    "                comp_ga = np.load(f'{fff}/X_comp_GA.npy')*mask\n",
    "                \n",
    "                aw_n[:-1] = l2(comp*np.sqrt(fieldwise_area_weights), axis=field_dimensions)#/np.sqrt(pixels_per_field)\n",
    "                aw_n[-1] = l2(comp*np.sqrt(area_weights))#/np.sqrt(np.sum(pixels_per_field))\n",
    "                np.save(f'{fff}/{sector}aw_norm.npy', aw_n)\n",
    "                aw_en[:-1] = l2((comp - comp_ga)*np.sqrt(fieldwise_area_weights), axis=field_dimensions)#/np.sqrt(pixels_per_field)\n",
    "                aw_en[-1] = l2((comp - comp_ga)*np.sqrt(area_weights))#/np.sqrt(np.sum(pixels_per_field))\n",
    "                np.save(f'{fff}/{sector}aw_error_norm.npy', aw_en)\n",
    "                aw_nr = aw_en/aw_n\n",
    "                np.save(f'{fff}/{sector}aw_norm_ratio.npy', aw_nr)\n",
    "                \n",
    "                aw_nrgn = np.zeros(X_std.shape[-1] + 1)\n",
    "                comp *= X_std/X_std_fm\n",
    "                comp_ga *= X_std/X_std_fm\n",
    "                aw_nrgn[:-1] = l2((comp - comp_ga)*np.sqrt(fieldwise_area_weights), axis=field_dimensions)/l2((comp)*np.sqrt(fieldwise_area_weights), axis=field_dimensions)\n",
    "                aw_nrgn[-1] = l2((comp - comp_ga)*np.sqrt(area_weights))/l2((comp)*np.sqrt(area_weights))\n",
    "                np.save(f'{fff}/{sector}aw_norm_ratio_global_normalization.npy', aw_nrgn)\n",
    "            \n",
    "            item['total_aw_norm_ratio'] = aw_nr[-1]\n",
    "            item['fieldwise_aw_norm_ratio'] = aw_nr[:-1]\n",
    "            item['total_aw_norm_ratio_gn'] = aw_nrgn[-1]\n",
    "            item['fieldwise_aw_norm_ratio_gn'] = aw_nrgn[:-1]\n",
    "            item['total_aw_norm'] = aw_n[-1]\n",
    "            item['fieldwise_aw_norm'] = aw_n[:-1]\n",
    "            item['total_aw_error_norm'] = aw_en[-1]\n",
    "            item['fieldwise_aw_error_norm'] = aw_en[:-1]\n",
    "            \n",
    "            # compute relative error quantile:\n",
    "            try:\n",
    "                sorted_rel_error = np.load(f'{fff}/sorted_{sector}rel_error.npy')\n",
    "            except FileNotFoundError:\n",
    "                comp = reshaper.reshape(np.load(f'{fff}/X_comp.npy'))\n",
    "                comp_ga = reshaper.reshape(np.load(f'{fff}/X_comp_GA.npy'))\n",
    "                rel_error = np.abs((comp_ga/comp - 1))\n",
    "                sorted_rel_error = np.sort(rel_error)\n",
    "                np.save(f'{fff}/sorted_{sector}rel_error.npy', sorted_rel_error)\n",
    "            \n",
    "            item['sorted_rel_error'] = sorted_rel_error\n",
    "            \n",
    "            # compute significance of the error\n",
    "            try:\n",
    "                significance = reshaper.reshape(np.load(f'{fff}/{sector}significance.npy'))\n",
    "            except FileNotFoundError:\n",
    "                comp = reshaper.reshape(np.load(f'{fff}/X_comp.npy'))\n",
    "                comp_ga = reshaper.reshape(np.load(f'{fff}/X_comp_GA.npy'))\n",
    "                comp_std = reshaper.reshape(np.load(f'{fff}/X_comp_std.npy'))\n",
    "                significance = (comp_ga - comp)/comp_std # this is the raw significance without accounting for the number of independent events\n",
    "                np.save(f'{fff}/{sector}significance.npy', reshaper.inv_reshape(significance))\n",
    "                \n",
    "            item['significance'] = significance\n",
    "            item['sorted_significance'] = np.sort(np.abs(significance))\n",
    "            \n",
    "            # here we compute the number of independent events\n",
    "            Y = np.load(f'{fff}/Y.npy')\n",
    "            item['N_heatwaves'] = np.sum(Y)\n",
    "            item['N_heatwave_years'] = np.sum(np.max(Y, axis=1))\n",
    "            \n",
    "            N_ind_heatwaves = 0 # we consider two events independent if they are more than T days apart, i.e. they don't share data to compute A\n",
    "            for yr in Y:\n",
    "                i = 0\n",
    "                while i < len(yr):\n",
    "                    if yr[i]:\n",
    "                        i += item['T']\n",
    "                        N_ind_heatwaves += 1\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        \n",
    "            item['N_ind_heatwaves'] = N_ind_heatwaves\n",
    "            \n",
    "            N_decorr_heatwaves = 0 # we consider two events independent if they are more than T_decorr days apart, i.e. A has had time to decorrelate\n",
    "            # here we need to account for the possibility of having to skip more than one year\n",
    "            i = 0\n",
    "            for yr in Y:\n",
    "                if i >= len(yr): # we skip the year directly\n",
    "                    i = max(0,i-360)\n",
    "                    continue\n",
    "                while i < len(yr):\n",
    "                    if yr[i]:\n",
    "                        i += item['T_decorr']\n",
    "                        N_decorr_heatwaves += 1\n",
    "                    else:\n",
    "                        i += 1\n",
    "                i = max(0,i-360) # we finished this year, now we prepare for next year\n",
    "                        \n",
    "            item['N_decorr_heatwaves'] = N_decorr_heatwaves\n",
    "            \n",
    "            df.append(item.copy())\n",
    "            \n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# sort the dataset\n",
    "df.sort_values(['T', 'tau', 'percent'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84f010-397e-47f8-b527-2b460bcee544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into xarray:\n",
    "T_ind = np.sort(list(set(df['T']))) ; print(T_ind)\n",
    "tau_ind = np.sort(list(set(df['tau']))) ; print(tau_ind)\n",
    "percent_ind = np.sort(list(set(df['percent']))) ; print(percent_ind)\n",
    "nT, ntau, npercent = len(T_ind), len(tau_ind), len(percent_ind)\n",
    "npixels = len(sorted_rel_error)\n",
    "quantile = np.arange(npixels)/npixels\n",
    "pixel = np.arange(npixels)\n",
    "\n",
    "# field_names = ['t2m','zg500', 'mrso_filtered'] if root_folder.startswith('PLASIM') else ['zg500']\n",
    "field_names = ['zg500']\n",
    "\n",
    "nfields = len(field_names)\n",
    "fields_ind = np.arange(nfields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05423337-bcf5-4f81-b2e0-74066ba6ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_row = df.iloc[0] * np.nan\n",
    "for T in T_ind:\n",
    "    for tau in tau_ind:\n",
    "        for percent in percent_ind:\n",
    "            comp_rows = np.sum((df['T'] == T ) * (df['tau'] == tau) * (df['percent'] == percent))\n",
    "            if comp_rows == 1:\n",
    "                continue\n",
    "            elif comp_rows > 1:\n",
    "                print(f'DUPLICATE ROW at {(T,tau,percent) = }')\n",
    "                \n",
    "            print(f'Inserting NaNs at {(T,tau,percent) = }')\n",
    "            template_row['T'] = T\n",
    "            template_row['tau'] = tau\n",
    "            template_row['percent'] = percent\n",
    "            df.loc[len(df)] = template_row.copy()      \n",
    "            \n",
    "df.sort_values(['T', 'tau', 'percent'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afaddef-ecee-4665-9539-d0c16e28f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({\n",
    "    'area_weights':        xr.DataArray(reshaper.reshape(area_weights),\n",
    "                                        coords={'pixel': pixel},\n",
    "                                        attrs={'description': 'normalized grid cell area weights'},\n",
    "                                       ),\n",
    "    'r':                   xr.DataArray(np.array(df['r']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'Lag-1 autocorrelation coefficient of A'}\n",
    "                                       ).sel(tau=0,percent=5), # r depends only on T\n",
    "    'T_decorr':            xr.DataArray(np.array(df['T_decorr']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'Decorrelation time of A',\n",
    "                                               'formula': r'$T_{decorr} = \\left\\lceil\\frac{1+r}{1-r}\\right\\rceil$'\n",
    "                                              }\n",
    "                                       ).sel(tau=0,percent=5), # T_decorr depends only on T\n",
    "    'a':                   xr.DataArray(np.array(df['a']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'threshold on A for defining a heatwave'}\n",
    "                                       ).sel(tau=0), # threshold does not depend on tau\n",
    "    'total_norm':    xr.DataArray(np.array(df['total_norm']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': '||C||'}\n",
    "                                       ),\n",
    "    'fieldwise_norm':xr.DataArray(np.stack(df['fieldwise_norm']).reshape(nT,ntau,npercent,nfields),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'field': fields_ind\n",
    "                                               },\n",
    "                                        attrs={'description': '||C|| computed independently for each field'}\n",
    "                                       ),\n",
    "    'total_error_norm':    xr.DataArray(np.array(df['total_error_norm']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': '||C_ga - C||'}\n",
    "                                       ),\n",
    "    'fieldwise_error_norm':xr.DataArray(np.stack(df['fieldwise_error_norm']).reshape(nT,ntau,npercent,nfields),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'field': fields_ind\n",
    "                                               },\n",
    "                                        attrs={'description': '||C_ga - C|| computed independently for each field'}\n",
    "                                       ),\n",
    "    'total_norm_ratio':    xr.DataArray(np.array(df['total_norm_ratio']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': '||C_ga - C||/||C||'}\n",
    "                                       ),\n",
    "    'fieldwise_norm_ratio':xr.DataArray(np.stack(df['fieldwise_norm_ratio']).reshape(nT,ntau,npercent,nfields),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'field': fields_ind\n",
    "                                               },\n",
    "                                        attrs={'description': '||C_ga - C||/||C|| computed independently for each field'}\n",
    "                                       ),\n",
    "    'total_norm_ratio_gn': xr.DataArray(np.array(df['total_norm_ratio_gn']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': '||C_ga - C||/||C|| computed with global normalization instead of pixel-wise'}\n",
    "                                       ),\n",
    "    'fieldwise_norm_ratio_gn':xr.DataArray(np.stack(df['fieldwise_norm_ratio_gn']).reshape(nT,ntau,npercent,nfields),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'field': fields_ind\n",
    "                                               },\n",
    "                                        attrs={'description': '||C_ga - C||/||C|| computed independently for each field with global normalization instead of pixel-wise'}\n",
    "                                       ),\n",
    "    'total_aw_norm':    xr.DataArray(np.array(df['total_aw_norm']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'area weighted norm ||C||_w'}\n",
    "                                       ),\n",
    "    'fieldwise_aw_norm':xr.DataArray(np.stack(df['fieldwise_aw_norm']).reshape(nT,ntau,npercent,nfields),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'field': fields_ind\n",
    "                                               },\n",
    "                                        attrs={'description': 'area weighted norm ||C||_w computed independently for each field'}\n",
    "                                       ),\n",
    "    'total_aw_error_norm':    xr.DataArray(np.array(df['total_aw_error_norm']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'area weighted norm ||C_ga - C||_w'}\n",
    "                                       ),\n",
    "    'fieldwise_aw_error_norm':xr.DataArray(np.stack(df['fieldwise_aw_error_norm']).reshape(nT,ntau,npercent,nfields),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'field': fields_ind\n",
    "                                               },\n",
    "                                        attrs={'description': 'area weighted norm ||C_ga - C||_w computed independently for each field'}\n",
    "                                       ),\n",
    "    'total_aw_norm_ratio':    xr.DataArray(np.array(df['total_aw_norm_ratio']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'area weighted norm ratio ||C_ga - C||_w/||C||_w'}\n",
    "                                       ),\n",
    "    'fieldwise_aw_norm_ratio':xr.DataArray(np.stack(df['fieldwise_aw_norm_ratio']).reshape(nT,ntau,npercent,nfields),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'field': fields_ind\n",
    "                                               },\n",
    "                                        attrs={'description': 'area weighted norm ratio ||C_ga - C||_w/||C||_w computed independently for each field'}\n",
    "                                       ),\n",
    "    'total_aw_norm_ratio_gn': xr.DataArray(np.array(df['total_aw_norm_ratio_gn']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'area weighted norm ratio ||C_ga - C||_w/||C||_w computed with global normalization instead of pixel-wise'}\n",
    "                                       ),\n",
    "    'fieldwise_aw_norm_ratio_gn':xr.DataArray(np.stack(df['fieldwise_aw_norm_ratio_gn']).reshape(nT,ntau,npercent,nfields),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'field': fields_ind\n",
    "                                               },\n",
    "                                        attrs={'description': 'area weighted norm ratio ||C_ga - C||_w/||C||_w computed independently for each field with global normalization instead of pixel-wise'}\n",
    "                                       ),\n",
    "    'relative_error':      xr.DataArray(np.stack(df['sorted_rel_error']).reshape(nT,ntau,npercent,npixels),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'quantile': quantile\n",
    "                                                },\n",
    "                                        attrs={'description': 'sorted values of |(C_ga[i]/C[i] - 1)|'}\n",
    "                                       ),\n",
    "    'significance':        xr.DataArray(np.stack(df['sorted_significance']).reshape(nT,ntau,npercent,npixels),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'quantile': quantile\n",
    "                                                },\n",
    "                                        attrs={'description': 'sorted values of |(C_ga[i] - C[i])/C_std[i]|'}\n",
    "                                       ),\n",
    "    'raw_significance':     xr.DataArray(np.stack(df['significance']).reshape(nT,ntau,npercent,npixels),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind),\n",
    "                                                'pixel': pixel\n",
    "                                                },\n",
    "                                        attrs={'description': 'values of (C_ga[i] - C[i])/C_std[i]'}\n",
    "                                       ),\n",
    "    'N_heatwaves':         xr.DataArray(np.array(df['N_heatwaves']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'number of heatwave days'}\n",
    "                                       ).sel(tau=0), # number of heatwaves does not depend on tau,\n",
    "    'N_heatwave_years':    xr.DataArray(np.array(df['N_heatwave_years']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'number of years with at least one heatwave day'}\n",
    "                                       ).sel(tau=0), # number of heatwaves does not depend on tau,\n",
    "    'N_ind_heatwaves':     xr.DataArray(np.array(df['N_ind_heatwaves']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'number of independent heatwave days, i.e. that are at least T days apart'}\n",
    "                                       ).sel(tau=0), # number of heatwaves does not depend on tau,\n",
    "    'N_decorr_heatwaves':  xr.DataArray(np.array(df['N_decorr_heatwaves']).reshape(nT,ntau,npercent),\n",
    "                                        coords={'T': np.array(T_ind),\n",
    "                                                'tau': np.array(tau_ind),\n",
    "                                                'percent': np.array(percent_ind)\n",
    "                                               },\n",
    "                                        attrs={'description': 'number of decorrelated heatwave days, i.e. that are at least T_decorr days apart'}\n",
    "                                       ).sel(tau=0), # number of heatwaves does not depend on tau,\n",
    "    },\n",
    "    attrs={\n",
    "        'description': 'metrics for comparisons between composite maps computed on the data and with the gaussian approximation',\n",
    "        # 'dataset': 'PLASIM - 80 years',\n",
    "        'dataset': 'ERA5'\n",
    "        'sector': f\"{sector.strip('_')}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# set attributes for coordinates of the dataset\n",
    "ds['T'].attrs = {'description': 'heatwave duration (A is computed as the forward T-day running mean)'}\n",
    "ds['tau'].attrs = {'description': 'time delay from the first day of the heatwave'}\n",
    "ds['percent'].attrs = {'description': 'the threshold `a` on A that defines heatwaves is such that `percent` of the data will be above `a`'}\n",
    "ds['quantile'].attrs = {'description': 'np.arange(`npixels`)/`npixels`'}\n",
    "ds['field'].attrs = {'description': 'index for the climate variables studied', 'field_names': ', '.join(field_names)}\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13afc16e-8c6d-4ff9-bd00-cc5c18f5f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(f'{root_folder}/metrics_.nc')\n",
    "os.rename(f'{root_folder}/metrics_.nc', f'{root_folder}/{sector}metrics.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7597e2c-1f52-429d-88c7-1f3cee31e8f1",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68978d91-2bf3-44c6-a854-acb0fa09f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(f'{root_folder}/{sector}metrics.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1487cb-ac48-443f-9855-6d4fded469a5",
   "metadata": {},
   "source": [
    "### Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9648f-c537-4912-8852-1737182c1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = ds['total_aw_norm_ratio'].sel(percent=5, \n",
    "                                 # tau=[0,5,10,15,20,25,30],\n",
    "                                 tau = np.arange(0,31,3),\n",
    "                                        # field=1,\n",
    "                                )\n",
    "\n",
    "xlabel = r'$\\tau$ [days]'\n",
    "ylabel = r'$T$ [days]'\n",
    "\n",
    "_ = tbl.table(sel.data, sel['tau'].data, sel['T'].data, color_range=(0.2,0.9), xlabel=xlabel, ylabel=ylabel, title='Norm ratio', num=5, figsize=(7,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7314b-b10c-4a05-8d0d-7b99aa9bb649",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tbl.tex_table(sel.data, sel['tau'].data, sel['T'].data, color_range=(0.2,0.9), xlabel=xlabel, ylabel=ylabel, title='Norm ratio', close_left=False)\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f52d4-dea3-4b3e-9756-df4e5e1fe60d",
   "metadata": {},
   "source": [
    "### Equivalent of table 2 for ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed47dd6-7700-4d1f-a933-e0cca05a49b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_below_threshold(data, quantiles, threshold):\n",
    "    assert data.shape == quantiles.shape\n",
    "    iot = data.data >= threshold\n",
    "    i = np.argmax(iot)\n",
    "    if i == 0:\n",
    "        if np.sum(iot) == 0:\n",
    "            return np.array([1])\n",
    "        return np.array([0])\n",
    "    return quantiles[i-1:i] # the first value of quantiles for which data overcomes threshold\n",
    "\n",
    "def xr_qbt(da:xr.DataArray, threshold:float):\n",
    "    return xr.apply_ufunc(quantile_below_threshold, da, da['quantile'], threshold,\n",
    "                          input_core_dims=[['quantile'], ['quantile'], []],\n",
    "                          exclude_dims=set(['quantile']),\n",
    "                          vectorize=True\n",
    "                         ).rename(f'Quantile of {da.name} below {threshold}')\n",
    "\n",
    "def xr_qat(da:xr.DataArray, threshold:float):\n",
    "    return (1 - xr_qbt(da, threshold)).rename(f'Quantile of {da.name} above {threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd3160-6729-4f4b-823c-02dfd42e161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = 2\n",
    "# sign = 1\n",
    "\n",
    "sA = ((np.abs(ds['raw_significance'])*np.sqrt(ds['N_heatwave_years']) > sign)*ds['area_weights']).sum('pixel').rename('yr_area_significance')\n",
    "# sA = ((np.abs(ds['raw_significance'])*np.sqrt(ds['N_decorr_heatwaves']) > sign)*ds['area_weights']).sum('pixel').rename('decorr_area_significance')\n",
    "sA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf051df1-016b-433c-8419-107608652bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = sA.sel(percent=5,\n",
    "            tau=np.arange(0,31,3),\n",
    "            )\n",
    "\n",
    "xlabel = r'$\\tau$ [days]'\n",
    "ylabel = r'$T$ [days]'\n",
    "\n",
    "_ = tbl.table(sel.data, sel['tau'].data, sel['T'].data, \n",
    "              xlabel=xlabel, ylabel=ylabel,\n",
    "              title = fr'Fraction of area with error above ${sign}\\sigma$',\n",
    "              color_range = (0.2,0.9),\n",
    "              text_digits=3,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0386ca-edb6-4dcf-b2bb-4cb15cbc3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tbl.tex_table(sel.data, sel['tau'].data, sel['T'].data, \n",
    "                  xlabel=xlabel, ylabel=ylabel,\n",
    "                  title = fr'Fraction of area with error above ${sign}\\sigma$',\n",
    "                  color_range = (0.2,0.9),\n",
    "                  close_left=False,\n",
    "                  text_digits=3,\n",
    "                 )\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc92c0-4220-4888-b3c0-bd92adeb82dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4ae6e1f-74b2-41a7-8b58-5c196c3cbbcd",
   "metadata": {},
   "source": [
    "### Figure 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd887b4f-f238-4f02-be43-76eeb1f82a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_purpose.cartopy_plots as cplt\n",
    "\n",
    "lon = np.load(f'{root_folder}/lon.npy')\n",
    "lon = cplt.monotonize_longitude(lon)\n",
    "lat = np.load(f'{root_folder}/lat.npy')\n",
    "LON, LAT = np.meshgrid(lon,lat)\n",
    "\n",
    "def retrieve_maps(T, tau, percent):\n",
    "    folder = f'{root_folder}/T{T}/tau{tau}/percent{percent}'\n",
    "    comp = np.load(f'{folder}/X_comp.npy')\n",
    "    comp_std = np.load(f'{folder}/X_comp_std.npy')\n",
    "    comp_ga = np.load(f'{folder}/X_comp_GA.npy')\n",
    "    \n",
    "    return comp, comp_ga, comp_std\n",
    "\n",
    "_,_,Xcstd = retrieve_maps(14,0,5)\n",
    "reshaper = ut.Reshaper(Xcstd != 0)\n",
    "print(reshaper.surviving_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db640a-382f-44ea-9c82-b088f46321c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657292c-def2-4df5-82ce-72ef39aa0d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = 14\n",
    "tau=0\n",
    "percent=5\n",
    "comp, comp_ga, comp_std = retrieve_maps(T,tau,percent)\n",
    "\n",
    "fig = cplt.mfp(LON, LAT, comp, figsize=(6,5), fig_num=8,colorbar='shared', titles=[r'$C_\\mathcal{D}$'], mx=1.05)[0].get_figure()\n",
    "\n",
    "fig.savefig(f'{HOME}ERA_comp.pdf')\n",
    "\n",
    "fig = cplt.mfp(LON, LAT, comp_ga, figsize=(6,5), fig_num=9,titles=[r'$C_\\mathcal{G}$'], mx=1.05)[0].get_figure()\n",
    "\n",
    "fig.savefig(f'{HOME}ERA_comp_ga.pdf')\n",
    "\n",
    "fig = cplt.mfp(LON, LAT, comp - comp_ga, figsize=(6,5), fig_num=10,titles=[r'$C_\\mathcal{D} - C_\\mathcal{G}$'],mx=0.25)[0].get_figure()\n",
    "\n",
    "fig.savefig(f'{HOME}ERA_comp_error.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c77fc-14fa-43bb-9bfb-86ef2a9f9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_purpose.cartopy_plots as cplt\n",
    "\n",
    "lon = np.load(f'{root_folder}/lon.npy')\n",
    "lon = cplt.monotonize_longitude(lon)\n",
    "lat = np.load(f'{root_folder}/lat.npy')\n",
    "LON, LAT = np.meshgrid(lon,lat)\n",
    "\n",
    "def retrieve_maps(T, tau, percent):\n",
    "    folder = f'{root_folder}/T{T}/tau{tau}/percent{percent}'\n",
    "    comp = np.load(f'{folder}/X_comp.npy')\n",
    "    comp_std = np.load(f'{folder}/X_comp_std.npy')\n",
    "    comp_ga = np.load(f'{folder}/X_comp_GA.npy')\n",
    "    \n",
    "    return comp, comp_ga, comp_std\n",
    "\n",
    "_,_,Xcstd = retrieve_maps(14,0,5)\n",
    "reshaper = ut.Reshaper(Xcstd != 0)\n",
    "print(reshaper.surviving_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a7177-3653-4b0d-92b1-739b48d2c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "taus = [0,2,4,6]\n",
    "percent = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8d1df-ec99-4ee4-8252-d11a8888f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.round(ds.total_aw_norm.sel(T=T, percent = percent, tau = taus).data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bbb994-30a5-45fd-92db-591861d69568",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea39105-3d3b-4720-8168-2b42420af134",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tau in enumerate(taus):\n",
    "    comp, comp_ga, comp_std = retrieve_maps(T, tau, percent)\n",
    "    axs = cplt.mfp(LON, LAT, comp_ga/norms[i], one_fig_layout=110, figsize=(5,5), fig_num=8+i, colorbar='individual', titles=f'|C|={norms[i]}',mx=4,put_colorbar=False,)\n",
    "    fig = axs[0].get_figure()\n",
    "    fig.savefig(f'{HOME}ERA5_C_T{T}_tau{tau}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
